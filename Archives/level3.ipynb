{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (3.9.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\prajwal\\advance mcq generator\\env\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.0)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, requests, multiprocess, fsspec, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed datasets-3.1.0 fsspec-2024.9.0 multiprocess-0.70.16 requests-2.32.3 xxhash-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "simplerllm 0.3.1.1 requires Requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Prajwal\\Advance MCQ Generator\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 'huggingface_dataset_id' (train split)...\n",
      "An error occurred: Dataset 'huggingface_dataset_id' doesn't exist on the Hub or cannot be accessed.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def download_parquet_dataset(dataset_name: str, save_path: str, split: str = \"train\"):\n",
    "    \"\"\"\n",
    "    Downloads a dataset from Hugging Face stored in Parquet format and saves it locally.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): The name of the dataset on Hugging Face.\n",
    "        save_path (str): The local directory path where the dataset should be saved.\n",
    "        split (str): The dataset split to download (default: \"train\").\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        print(f\"Downloading dataset '{dataset_name}' ({split} split)...\")\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "        \n",
    "        # Save the dataset to the specified path\n",
    "        save_file = f\"{save_path}/{dataset_name}_{split}.parquet\"\n",
    "        # dataset.to_parquet(save_file)\n",
    "        return dataset\n",
    "        \n",
    "        # print(f\"Dataset saved at {save_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your-dataset-name' with the dataset ID from Hugging Face\n",
    "    # Replace 'your/local/path' with the directory to save the dataset\n",
    "    download_parquet_dataset(\"huggingface_dataset_id\", \"your/local/path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 'rajpurkar/squad' (validation split)...\n"
     ]
    }
   ],
   "source": [
    "open_ended = download_parquet_dataset(dataset_name=\"rajpurkar/squad\", save_path=\"data/datasets/open_ended.parquet\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 10570\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 'allenai/qasc' (test split)...\n"
     ]
    }
   ],
   "source": [
    "mcq = download_parquet_dataset(dataset_name=\"allenai/qasc\", save_path=\"data/datasets/mcq.parquet\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choices', 'answerKey', 'fact1', 'fact2', 'combinedfact', 'formatted_question'],\n",
       "    num_rows: 8134\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_open = # context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_open = \"\"\"\n",
    "question: {question} \\n\n",
    "answer: {answer}\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def convert_to_instruction_format_open(dataset: str, instruction: str, split: str = \"train\"):\n",
    "    \"\"\"\n",
    "    Converts a Hugging Face dataset to a new format with features: ['instruction', 'input', 'output'].\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): The name of the dataset on Hugging Face.\n",
    "        instruction (str): The instruction text to include for all rows.\n",
    "        split (str): The dataset split to load and transform (default: \"train\").\n",
    "\n",
    "    Returns:\n",
    "        new_dataset: A transformed dataset in the new format.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    \n",
    "    # Define the conversion function\n",
    "    def transform_row(row):\n",
    "        input_text = f\"Title: {row['title']}\\nContext: {row['context']}\"\n",
    "        output_text =f\"\"\"\n",
    "question: {row[\"question\"]} \\n\n",
    "answer: {(row[\"answers\"][\"text\"][0])}\n",
    "\"\"\" \n",
    "        return {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text,\n",
    "            \"id\": row['id']\n",
    "        }\n",
    "    \n",
    "    # Apply the transformation\n",
    "    print(\"Converting dataset...\")\n",
    "    new_data = dataset.map(transform_row, remove_columns=['id', 'title', 'context', 'question', 'answers'])\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10570/10570 [00:00<00:00, 16813.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "template_ope= \"\"\"Given the following context create me open ended question-answer pair\"\"\"\n",
    "\n",
    "new_dataset = convert_to_instruction_format_open(open_ended, instruction=template_ope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'instruction', 'input', 'output'],\n",
       "    num_rows: 10570\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-tuned dataset:\n",
      "{'id': '56be4db0acb8001400a502ec', 'instruction': 'Given the following context create me open ended question-answer pair', 'input': 'Title: Super_Bowl_50\\nContext: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'output': '\\nquestion: Which NFL team represented the AFC at Super Bowl 50? \\n\\nanswer: Denver Broncos\\n'}\n",
      "{'id': '56be4db0acb8001400a502ed', 'instruction': 'Given the following context create me open ended question-answer pair', 'input': 'Title: Super_Bowl_50\\nContext: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'output': '\\nquestion: Which NFL team represented the NFC at Super Bowl 50? \\n\\nanswer: Carolina Panthers\\n'}\n",
      "{'id': '56be4db0acb8001400a502ee', 'instruction': 'Given the following context create me open ended question-answer pair', 'input': 'Title: Super_Bowl_50\\nContext: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'output': '\\nquestion: Where did Super Bowl 50 take place? \\n\\nanswer: Santa Clara, California\\n'}\n",
      "{'id': '56be4db0acb8001400a502ef', 'instruction': 'Given the following context create me open ended question-answer pair', 'input': 'Title: Super_Bowl_50\\nContext: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'output': '\\nquestion: Which NFL team won Super Bowl 50? \\n\\nanswer: Denver Broncos\\n'}\n",
      "{'id': '56be4db0acb8001400a502f0', 'instruction': 'Given the following context create me open ended question-answer pair', 'input': 'Title: Super_Bowl_50\\nContext: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'output': '\\nquestion: What color was used to emphasize the 50th anniversary of the Super Bowl? \\n\\nanswer: gold\\n'}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "m = 5\n",
    "print(\"Instruction-tuned dataset:\")\n",
    "top_m = list(itertools.islice(new_dataset, m))\n",
    "for j in top_m:\n",
    "  print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 845.84ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10815959"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.to_parquet(\"./dataset/val_open.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choices', 'answerKey', 'fact1', 'fact2', 'combinedfact', 'formatted_question'],\n",
       "    num_rows: 926\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_mcq= \"\"\"Given the following context create me MCQ questions with options and answer pair\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def convert_to_instruction_format_mcq(dataset: str, instruction: str, split: str = \"train\"):\n",
    "    \"\"\"\n",
    "    Converts a Hugging Face dataset to a new format with features: ['instruction', 'input', 'output'].\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): The name of the dataset on Hugging Face.\n",
    "        instruction (str): The instruction text to include for all rows.\n",
    "        split (str): The dataset split to load and transform (default: \"train\").\n",
    "\n",
    "    Returns:\n",
    "        new_dataset: A transformed dataset in the new format.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    \n",
    "    # Define the conversion function\n",
    "    def transform_row(row):\n",
    "        input_text = row['combinedfact']\n",
    "        output_text =f\"\"\"\n",
    "question: {row[\"formatted_question\"]} \\n\n",
    "answer: {(row[\"answerKey\"])}\n",
    "\"\"\" \n",
    "        return {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text,\n",
    "            \"id\": row['id']\n",
    "        }\n",
    "    \n",
    "    # Apply the transformation\n",
    "    print(\"Converting dataset...\")\n",
    "    new_data = dataset.map(transform_row, remove_columns=['question', 'choices', 'answerKey', 'fact1', 'fact2', 'combinedfact', 'formatted_question'])\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 920/920 [00:00<00:00, 17795.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mcq_updated = convert_to_instruction_format_mcq(dataset=mcq, instruction=template_mcq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'instruction', 'input', 'output'],\n",
       "    num_rows: 920\n",
       "})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'instruction', 'input', 'output'],\n",
       "    num_rows: 10570\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-tuned dataset:\n",
      "{'id': '3C44YUNSI1OBFBB8D36GODNOZN9DPA', 'instruction': 'Given the following context create me MCQ questions with options and answer pair', 'input': '', 'output': '\\nquestion: What type of birth do therian mammals have? (A) Anthax (B) under water (C) uterus (D) wombs (E) two (F) moles (G) live (H) embryo \\n\\nanswer: \\n'}\n",
      "{'id': '3B1NLC6UGZVERVLZFT7OUYQLD1SGPZ', 'instruction': 'Given the following context create me MCQ questions with options and answer pair', 'input': '', 'output': '\\nquestion: By what time had mouse-sized viviparous mammals evolved? (A) Corvidae (B) arthropods (C) birds (D) backbones (E) keratin (F) Jurassic (G) front paws (H) Parakeets. \\n\\nanswer: \\n'}\n",
      "{'id': '3QRYMNZ7FYGITFVSJET3PS0F4S0NT9', 'instruction': 'Given the following context create me MCQ questions with options and answer pair', 'input': '', 'output': \"\\nquestion: What does a plant's skin do? (A) Reduced friction (B) causes infection (C) vital to a good life (D) prevents water loss (E) camouflage from consumers (F) Protection against predators (G) spur the growth of the plant (H) a smooth surface \\n\\nanswer: \\n\"}\n",
      "{'id': '336KAV9KYQRILF5T71II5LPW6IJ2YE', 'instruction': 'Given the following context create me MCQ questions with options and answer pair', 'input': '', 'output': '\\nquestion: The layer of protection for preventing water loss, abrasions, and infections is located on the: (A) large back muscles (B) upper epidermis (C) therian mammals (D) underground (E) calcium and protein (F) A hinged joint (G) vital to a good life (H) upper arm muscles \\n\\nanswer: \\n'}\n",
      "{'id': '3NJM2BJS4W51AJ5UD7B54756E49CPJ', 'instruction': 'Given the following context create me MCQ questions with options and answer pair', 'input': '', 'output': '\\nquestion: What is protected, preventing water loss, abrasions, infections, and damage from toxins? (A) VOCs and NOx (B) protection (C) penguins (D) keratin (E) hornworts (F) Shelter (G) the nail (H) organisms \\n\\nanswer: \\n'}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "m = 5\n",
    "print(\"Instruction-tuned dataset:\")\n",
    "top_m = list(itertools.islice(mcq_updated, m))\n",
    "for j in top_m:\n",
    "  print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 499.86ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "287267"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq_updated.to_parquet(\"./dataset/test_mcq.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset\n",
    "import os\n",
    "\n",
    "def concatenate_parquet_with_hf(parquet_files: str, output_file: str = None):\n",
    "    \"\"\"\n",
    "    Reads multiple Parquet files as Hugging Face datasets and concatenates them vertically.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): The folder containing the Parquet files.\n",
    "        output_file (str): Optional. The path to save the concatenated dataset.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: The concatenated Hugging Face dataset.\n",
    "    \"\"\"\n",
    "    if not parquet_files:\n",
    "        raise ValueError(\"No Parquet files found in the specified folder.\")\n",
    "    \n",
    "    print(f\"Found {len(parquet_files)} files. Reading and concatenating...\")\n",
    "\n",
    "    # Load each Parquet file as a Hugging Face dataset\n",
    "    datasets = [load_dataset(\"parquet\", data_files=file)[\"train\"] for file in parquet_files]\n",
    "    \n",
    "    # Concatenate the datasets\n",
    "    concatenated_dataset = concatenate_datasets(datasets)\n",
    "    \n",
    "    # Optionally save the concatenated dataset\n",
    "    if output_file:\n",
    "        concatenated_dataset.to_parquet(output_file)\n",
    "        print(f\"Concatenated dataset saved to {output_file}\")\n",
    "    \n",
    "    return concatenated_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files. Reading and concatenating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 96/96 [00:00<00:00, 732.84ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated dataset saved to ./dataset/train_merge.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_list = [r'dataset/train_mcq.parquet', r'dataset/train_open.parquet']\n",
    "\n",
    "train_merge = concatenate_parquet_with_hf(train_list, \"./dataset/train_merge.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 files. Reading and concatenating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:00<00:00, 721.12ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated dataset saved to ./dataset/test_merge.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_list = [r'dataset/test_mcq.parquet', r'dataset/val_mcq.parquet', r'dataset/val_open.parquet']\n",
    "\n",
    "train_merge = concatenate_parquet_with_hf(test_list, \"./dataset/test_merge.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
